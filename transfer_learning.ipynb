{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for Landmark Classification\n",
    "\n",
    "\n",
    "### Transfer learning\n",
    "\n",
    "In the previous notebook we have trained our own CNN and we got a certain performance. Let's see how hard it is to match that performance with transfer learning.\n",
    "\n",
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 0: Setting up\n",
    "\n",
    "The following cells make sure that your environment is setup correctly and check that your GPU is available and ready to go. You have to execute them every time you restart your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries for Python Pillow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.5).\n",
      "zlib1g-dev set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  libjpeg-turbo8-dev libjpeg8-dev libpng-tools\n",
      "The following NEW packages will be installed:\n",
      "  libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev libpng-dev libpng-tools\n",
      "0 upgraded, 5 newly installed, 0 to remove and 3 not upgraded.\n",
      "Need to get 443 kB of archives.\n",
      "After this operation, 1870 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libjpeg-turbo8-dev amd64 2.0.3-0ubuntu1.20.04.3 [238 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libjpeg8-dev amd64 8c-2ubuntu8 [1552 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libjpeg-dev amd64 8c-2ubuntu8 [1546 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libpng-dev amd64 1.6.37-2 [175 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libpng-tools amd64 1.6.37-2 [26.1 kB]\n",
      "Fetched 443 kB in 0s (5214 kB/s)        \u001b[0m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libjpeg-turbo8-dev:amd64.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../libjpeg-turbo8-dev_2.0.3-0ubuntu1.20.04.3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libjpeg-turbo8-dev:amd64 (2.0.3-0ubuntu1.20.04.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libjpeg8-dev:amd64.\n",
      "Preparing to unpack .../libjpeg8-dev_8c-2ubuntu8_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libjpeg8-dev:amd64 (8c-2ubuntu8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libjpeg-dev:amd64.\n",
      "Preparing to unpack .../libjpeg-dev_8c-2ubuntu8_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libjpeg-dev:amd64 (8c-2ubuntu8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libpng-dev:amd64.\n",
      "Preparing to unpack .../libpng-dev_1.6.37-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libpng-dev:amd64 (1.6.37-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libpng-tools.\n",
      "Preparing to unpack .../libpng-tools_1.6.37-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libpng-tools (1.6.37-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Setting up libjpeg-turbo8-dev:amd64 (2.0.3-0ubuntu1.20.04.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libpng-tools (1.6.37-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libpng-dev:amd64 (1.6.37-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libjpeg8-dev:amd64 (8c-2ubuntu8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [##############################################............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libjpeg-dev:amd64 (8c-2ubuntu8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Processing triggers for man-db (2.9.1-1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!sudo apt install zlib1g-dev libjpeg-dev libpng-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erase any checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./checkpoionts/*.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless==4.5.3.56\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp39-cp39-manylinux2014_x86_64.whl (37.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.1/37.1 MB 48.8 MB/s eta 0:00:00\n",
      "Collecting matplotlib==3.4.3\n",
      "  Downloading matplotlib-3.4.3-cp39-cp39-manylinux1_x86_64.whl (10.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 116.9 MB/s eta 0:00:00\n",
      "Collecting numpy==1.21.2\n",
      "  Downloading numpy-1.21.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.8/15.8 MB 89.6 MB/s eta 0:00:00\n",
      "Collecting pillow==7.0.0\n",
      "  Downloading Pillow-7.0.0.tar.gz (38.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.2/38.2 MB 37.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bokeh==2.1.1\n",
      "  Downloading bokeh-2.1.1.tar.gz (19.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/19.3 MB 82.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch==1.11.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 750.6/750.6 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting torchvision==0.12.0\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-manylinux1_x86_64.whl (21.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 58.5 MB/s eta 0:00:00\n",
      "Collecting tqdm==4.63.0\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.6/76.6 kB 25.1 MB/s eta 0:00:00\n",
      "Collecting ipywidgets==7.6.5\n",
      "  Downloading ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.8/121.8 kB 38.1 MB/s eta 0:00:00\n",
      "Collecting livelossplot==0.5.4\n",
      "  Downloading livelossplot-0.5.4-py3-none-any.whl (22 kB)\n",
      "Collecting pytest==7.1.1\n",
      "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 62.7 MB/s eta 0:00:00\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 112.1 MB/s eta 0:00:00\n",
      "Collecting seaborn==0.11.2\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 65.6 MB/s eta 0:00:00\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 119.1 MB/s eta 0:00:00\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 20.6 MB/s eta 0:00:00\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 33.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pillow, bokeh\n",
      "  Building wheel for pillow (setup.py): started\n",
      "  Building wheel for pillow (setup.py): finished with status 'done'\n",
      "  Created wheel for pillow: filename=Pillow-7.0.0-cp39-cp39-linux_x86_64.whl size=1144543 sha256=a1c2afca3ac30173086d1659caa84b470bfd9c2349aaac085c94154ae4ebd0e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/a7/5b/0f37c54d8e265bc05ec656639d469c87bbc9a95dabac1e6d2a\n",
      "  Building wheel for bokeh (setup.py): started\n",
      "  Building wheel for bokeh (setup.py): finished with status 'done'\n",
      "  Created wheel for bokeh: filename=bokeh-2.1.1-py3-none-any.whl size=9257179 sha256=6428a62d9c8fc35d2deb92c96b8c72e443e85af2a3dc7189ace01f4c31ac93fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/c1/04/8c5bbdff105516c6ff2f56317b5116763e0d02d906ea5c9fc7\n",
      "Successfully built pillow bokeh\n",
      "Installing collected packages: tqdm, torch, py, pillow, numpy, attrs, torchvision, pytest, pandas, opencv-python-headless, matplotlib, bokeh, seaborn, livelossplot, widgetsnbextension, ipywidgets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu116\n",
      "    Uninstalling torchvision-0.13.1+cu116:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.2.1\n",
      "    Uninstalling pytest-7.2.1:\n",
      "      Successfully uninstalled pytest-7.2.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.0\n",
      "    Uninstalling pandas-1.5.0:\n",
      "      Successfully uninstalled pandas-1.5.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.6.1\n",
      "    Uninstalling matplotlib-3.6.1:\n",
      "      Successfully uninstalled matplotlib-3.6.1\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.0\n",
      "    Uninstalling seaborn-0.12.0:\n",
      "      Successfully uninstalled seaborn-0.12.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.5\n",
      "    Uninstalling widgetsnbextension-4.0.5:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.5\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.0.2\n",
      "    Uninstalling ipywidgets-8.0.2:\n",
      "      Successfully uninstalled ipywidgets-8.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\n",
      "imageio 2.25.0 requires pillow>=8.3.2, but you have pillow 7.0.0 which is incompatible.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 22.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-22.2.0 bokeh-2.1.1 ipywidgets-7.6.5 livelossplot-0.5.4 matplotlib-3.4.3 numpy-1.21.2 opencv-python-headless-4.5.3.56 pandas-1.3.5 pillow-7.0.0 py-1.11.0 pytest-7.1.1 seaborn-0.11.2 torch-1.11.0 torchvision-0.12.0 tqdm-4.63.0 widgetsnbextension-3.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n",
      "Downloading and unzipping https://udacity-dlnfd.s3-us-west-1.amazonaws.com/datasets/landmark_images.zip. This will take a while...\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mean: 100%|███████████████████████| 6246/6246 [01:02<00:00, 99.73it/s]\n",
      "Computing std: 100%|████████████████████████| 6246/6246 [01:34<00:00, 65.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.helpers import setup_env\n",
    "\n",
    "# If running locally, this will download dataset (make sure you have at \n",
    "# least 2 Gb of space on your hard drive)\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 1: Create transfer learning architecture\n",
    "\n",
    "Open the file `src/transfer.py` and complete the `get_model_transfer_learning` function. When you are done, execute this test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.16, pytest-7.1.1, pluggy-1.0.0 -- /usr/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /notebooks/Deep-Learning-Landmarks\n",
      "plugins: anyio-3.6.2\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "src/transfer.py::test_get_model_transfer_learning \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "  /usr/local/lib/python3.9/dist-packages/matplotlib/__init__.py:152: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    if LooseVersion(module.__version__) < minver:\n",
      "\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "src/transfer.py::test_get_model_transfer_learning\n",
      "  /usr/local/lib/python3.9/dist-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    other = LooseVersion(other)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m10 warnings\u001b[0m\u001b[33m in 2.03s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv src/transfer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 2: Train, validation and test\n",
    "\n",
    "Let's train our transfer learning model! Let's start defining the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # size of the minibatch for stochastic gradient descent (or Adam)\n",
    "valid_size = 0.2  # fraction of the training data to reserve for validation\n",
    "num_epochs = 200  # number of epochs for training\n",
    "num_classes = 50  # number of classes. Do not change this\n",
    "learning_rate = 0.001  # Learning rate for SGD (or Adam)\n",
    "opt = 'adam'      # optimizer. 'sgd' or 'adam'\n",
    "weight_decay = 0.0 # regularization. Increase this to combat overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing cached mean and std\n",
      "Dataset mean: tensor([0.4638, 0.4725, 0.4687]), std: tensor([0.2697, 0.2706, 0.3017])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA RTX A5000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA RTX A5000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "Training:   0%|                                          | 0/63 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m get_optimizer(\n\u001b[1;32m     16\u001b[0m     model_transfer,\n\u001b[1;32m     17\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mopt,\n\u001b[1;32m     19\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_loss()\n\u001b[0;32m---> 23\u001b[0m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_transfer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/model_transfer.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteractive_tracking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/Deep-Learning-Landmarks/src/train.py:121\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(data_loaders, model, optimizer, loss, n_epochs, save_path, interactive_tracking)\u001b[0m\n\u001b[1;32m    117\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m valid_one_epoch(data_loaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, loss)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# print training/validation statistics\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/Deep-Learning-Landmarks/src/train.py:42\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_dataloader, model, optimizer, loss)\u001b[0m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 2. forward pass: compute predicted outputs by passing inputs to the model\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m output  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 3. calculate the loss\u001b[39;00m\n\u001b[1;32m     45\u001b[0m loss_value  \u001b[38;5;241m=\u001b[39m loss(output, target) \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py:283\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/resnet.py:267\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 267\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/batchnorm.py:148\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[1;32m    150\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from src.data import get_data_loaders\n",
    "from src.optimization import get_optimizer, get_loss\n",
    "from src.train import optimize\n",
    "from src.transfer import get_model_transfer_learning\n",
    "\n",
    "# Get a model using get_model_transfer_learning. Use one of the names reported here:\n",
    "# https://pytorch.org/vision/0.10/models.html\n",
    "# For example, if you want to load ResNet 18, use \"resnet18\"\n",
    "# NOTE: use the hyperparameters defined in the previous cell, do NOT copy/paste the\n",
    "# values\n",
    "model_transfer = get_model_transfer_learning()# YOUR CODE HERE\n",
    "\n",
    "# train the model\n",
    "data_loaders = get_data_loaders(batch_size=batch_size)\n",
    "optimizer = get_optimizer(\n",
    "    model_transfer,\n",
    "    learning_rate=learning_rate,\n",
    "    optimizer=opt,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "loss = get_loss()\n",
    "\n",
    "optimize(\n",
    "    data_loaders,\n",
    "    model_transfer,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    n_epochs=num_epochs,\n",
    "    save_path=\"checkpoints/model_transfer.pt\",\n",
    "    interactive_tracking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static_images/icons/noun-question-mark-869751.png\" alt=\"?\" style=\"width:25px\"/> __Question:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static_images/icons/noun-answer-3361020.png\" alt=\">\" style=\"width:25px\"/>  __Answer:__ I decided to use ResNet18 for the base of my model, since it performs fairly well on ImageNet and is not too large of a model. Also, since ResNet18 was trained for the ImageNet task, it is a good model to use for this landmark classificaiton task, since both ImageNet and this landmark task use images of natural scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with the hyperparameters and see which performance you can get on the validation set. You should get at least 60% for a passing grade, but a good model choice and a good training strategy could get you up to 80% or so. Let's see how close you can get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 3: Test the Model\n",
    "\n",
    "Try out your model on the test dataset of landmark images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60% and matches more or less what you got on the validation set (otherwise you're overfitting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████| 20/20 [00:03<00:00,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.104552\n",
      "\n",
      "\n",
      "Test Accuracy: 69% (873/1250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1045519739389418"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from src.train import one_epoch_test\n",
    "from src.transfer import get_model_transfer_learning\n",
    "\n",
    "model_transfer = get_model_transfer_learning(\"resnet18\", n_classes=num_classes)\n",
    "# Load saved weights\n",
    "model_transfer.load_state_dict(torch.load('checkpoints/model_transfer.pt'))\n",
    "\n",
    "one_epoch_test(data_loaders['test'], model_transfer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 4: Export using torchscript\n",
    "\n",
    "Now, just like we did with our original model, we export the best fit model using torchscript so that it can be used in our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predictor import Predictor\n",
    "from src.helpers import compute_mean_and_std\n",
    "\n",
    "# First let's get the class names from our data loaders\n",
    "class_names = data_loaders[\"train\"].dataset.classes\n",
    "\n",
    "# Then let's move the model_transfer to the CPU\n",
    "# (we don't need GPU for inference)\n",
    "model_transfer = model_transfer.cpu()\n",
    "# Let's make sure we use the right weights by loading the\n",
    "# best weights we have found during training\n",
    "# NOTE: remember to use map_location='cpu' so the weights\n",
    "# are loaded on the CPU (and not the GPU)\n",
    "model_transfer.load_state_dict(\n",
    "    torch.load(\"checkpoints/model_transfer.pt\", map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "# Let's wrap our model using the predictor class\n",
    "mean, std = compute_mean_and_std()\n",
    "predictor = Predictor(model_transfer, class_names, mean, std).cpu()\n",
    "\n",
    "# Export using torch.jit.script\n",
    "scripted_predictor = torch.jit.script(predictor)\n",
    "scripted_predictor.save(\"checkpoints/transfer_exported.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.predictor import predictor_test\n",
    "from src.helpers import plot_confusion_matrix\n",
    "\n",
    "model_reloaded = torch.jit.load(\"checkpoints/transfer_exported.pt\")\n",
    "\n",
    "pred, truth = predictor_test(data_loaders['test'], model_reloaded)\n",
    "\n",
    "plot_confusion_matrix(pred, truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
